{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Project Overview & Environment Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup & Library Imports\n",
    "\n",
    "# Data manipulation and numerical operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Processing and feature engineering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Reproducibility\n",
    "from numpy.random import default_rng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we initialize the environment with the necessary tools for data processing, ensemble modeling (Random Forest, Gradient Boosting), and hyperparameter tuning using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully. \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calls</th>\n",
       "      <th>minutes</th>\n",
       "      <th>messages</th>\n",
       "      <th>mb_used</th>\n",
       "      <th>is_ultra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.0</td>\n",
       "      <td>311.90</td>\n",
       "      <td>83.0</td>\n",
       "      <td>19915.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85.0</td>\n",
       "      <td>516.75</td>\n",
       "      <td>56.0</td>\n",
       "      <td>22696.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77.0</td>\n",
       "      <td>467.66</td>\n",
       "      <td>86.0</td>\n",
       "      <td>21060.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106.0</td>\n",
       "      <td>745.53</td>\n",
       "      <td>81.0</td>\n",
       "      <td>8437.39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66.0</td>\n",
       "      <td>418.74</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14502.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   calls  minutes  messages   mb_used  is_ultra\n",
       "0   40.0   311.90      83.0  19915.42         0\n",
       "1   85.0   516.75      56.0  22696.96         0\n",
       "2   77.0   467.66      86.0  21060.45         0\n",
       "3  106.0   745.53      81.0   8437.39         1\n",
       "4   66.0   418.74       1.0  14502.75         0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Acquisition & Exploratory Analysis\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('../data/users_behavior.csv')\n",
    "    print(\"Dataset loaded successfully. \\n\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Dataset file not found.\")\n",
    "\n",
    "# Displaying the first few records to understand the structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of user behavior data from Megaline, including calls, messages, and internet usage, with the target variable 'is_ultra' indicating the current tariff plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3214 entries, 0 to 3213\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   calls     3214 non-null   float64\n",
      " 1   minutes   3214 non-null   float64\n",
      " 2   messages  3214 non-null   float64\n",
      " 3   mb_used   3214 non-null   float64\n",
      " 4   is_ultra  3214 non-null   int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 125.7 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calls</th>\n",
       "      <th>minutes</th>\n",
       "      <th>messages</th>\n",
       "      <th>mb_used</th>\n",
       "      <th>is_ultra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3214.000000</td>\n",
       "      <td>3214.000000</td>\n",
       "      <td>3214.000000</td>\n",
       "      <td>3214.000000</td>\n",
       "      <td>3214.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>63.038892</td>\n",
       "      <td>438.208787</td>\n",
       "      <td>38.281269</td>\n",
       "      <td>17207.673836</td>\n",
       "      <td>0.306472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>33.236368</td>\n",
       "      <td>234.569872</td>\n",
       "      <td>36.148326</td>\n",
       "      <td>7570.968246</td>\n",
       "      <td>0.461100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>274.575000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>12491.902500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>62.000000</td>\n",
       "      <td>430.600000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>16943.235000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>82.000000</td>\n",
       "      <td>571.927500</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>21424.700000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>244.000000</td>\n",
       "      <td>1632.060000</td>\n",
       "      <td>224.000000</td>\n",
       "      <td>49745.730000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             calls      minutes     messages       mb_used     is_ultra\n",
       "count  3214.000000  3214.000000  3214.000000   3214.000000  3214.000000\n",
       "mean     63.038892   438.208787    38.281269  17207.673836     0.306472\n",
       "std      33.236368   234.569872    36.148326   7570.968246     0.461100\n",
       "min       0.000000     0.000000     0.000000      0.000000     0.000000\n",
       "25%      40.000000   274.575000     9.000000  12491.902500     0.000000\n",
       "50%      62.000000   430.600000    30.000000  16943.235000     0.000000\n",
       "75%      82.000000   571.927500    57.000000  21424.700000     1.000000\n",
       "max     244.000000  1632.060000   224.000000  49745.730000     1.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspecting data types and non-null counts\n",
    "df.info()\n",
    "\n",
    "# Statistical summary of numerical features\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Data Assessment**\n",
    "\n",
    "* **Data Integrity:** The dataset contains no missing values across any of the columns. This allows us to proceed directly to modeling without the need for imputation.\n",
    "\n",
    "* **Feature Characteristics**: * Activity Metrics: There is a significant variance in user activity.\n",
    "\n",
    "* **Target Distribution**: The `is_ultra` column is our binary target. An initial check (via value_counts) would be necessary to confirm if there is a class imbalance between the \"Smart\" and \"Ultra\" plans, which will influence our choice of evaluation metrics.\n",
    "\n",
    "* **Variable Types**: All features are numerical (float64 or int64), which is ideal for machine learning algorithms. However, feature scaling will be considered for distance-based models like Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Feature Engineering & Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:   1928 samples\n",
      "Validation set: 643 samples\n",
      "Test set:       643 samples\n"
     ]
    }
   ],
   "source": [
    "# Defining features (X) and target variable (y)\n",
    "X = df.drop(columns=['is_ultra'])\n",
    "y = df['is_ultra']\n",
    "\n",
    "# Feature Scaling\n",
    "# Using StandardScaler to normalize numerical features for better model convergence\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Dataset Splitting (60% Train, 20% Validation, 20% Test)\n",
    "# We use a two-step split and 'stratify' to maintain class balance across all sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 0.25 of the remaining 80% equals 20% of the original data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Verification of set dimensions\n",
    "print(f\"Training set:   {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_valid.shape[0]} samples\")\n",
    "print(f\"Test set:       {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methodology Note**: > To ensure a robust evaluation, the data was partitioned into Training (60%), Validation (20%), and Testing (20%) sets.\n",
    "\n",
    "* **Feature Scaling**: Since models like Logistic Regression are sensitive to the scale of input features, StandardScaler was applied to normalize the data.\n",
    "\n",
    "* **Stratification**: Given the potential for class imbalance in plan adoption, the stratify parameter was used to ensure that the ratio of 'Smart' vs 'Ultra' users remains consistent across all three subsets, preventing bias during model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Model Development**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Development & Hyperparameter Tuning\n",
    "\n",
    "def tune_and_evaluate(model, params, X_train, y_train, X_val, y_val, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Performs GridSearch cross-validation, tunes hyperparameters, \n",
    "    and evaluates the model on the validation set.\n",
    "    \"\"\"\n",
    "    print(f\"--- Tuning & Evaluating: {model_name} ---\")\n",
    "    \n",
    "    # 1. Hyperparameter tuning with GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model, \n",
    "        param_grid=params, \n",
    "        cv=5, \n",
    "        scoring='accuracy', \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # 2. Model Evaluation\n",
    "    val_predictions = best_model.predict(X_val)\n",
    "    val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "    \n",
    "    # 3. Display Results\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Training CV Accuracy: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"Validation Accuracy:  {val_accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report (Validation Set):\")\n",
    "    print(classification_report(y_val, val_predictions))\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1. 1st Model: Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tuning & Evaluating: Logistic Regression ---\n",
      "Best Parameters: {'C': 0.01, 'class_weight': None}\n",
      "Training CV Accuracy: 0.7510\n",
      "Validation Accuracy:  0.7465\n",
      "\n",
      "Classification Report (Validation Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.98      0.84       446\n",
      "           1       0.83      0.22      0.35       197\n",
      "\n",
      "    accuracy                           0.75       643\n",
      "   macro avg       0.78      0.60      0.59       643\n",
      "weighted avg       0.77      0.75      0.69       643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter grid\n",
    "log_reg_params = {\n",
    "    'C': [0.01, 0.1, 1, 10], \n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# Running the master function\n",
    "best_log_reg = tune_and_evaluate(\n",
    "    LogisticRegression(max_iter=2000, solver='liblinear'),\n",
    "    log_reg_params,\n",
    "    X_train, y_train,\n",
    "    X_valid, y_valid,\n",
    "    model_name=\"Logistic Regression\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interim Conclusions: Logistic Regression (Baseline)**\n",
    "The Logistic Regression model was implemented as a performance baseline. Based on the hyperparameter tuning and evaluation results, we can draw the following insights:\n",
    "\n",
    "* **Model Performance:** The model achieved a Validation Accuracy of 74.65%, which is consistent with the Training CV Accuracy (75.10%). This stability indicates that the model has good generalization capabilities and is not overfitting.\n",
    "\n",
    "* **Regularization:** The optimal hyperparameter C: 0.01 suggests a high level of regularization. This implies that the relationship between the features and the target variable is relatively simple or that the model is penalizing complex coefficients to maintain stability.\n",
    "\n",
    "* **Class Imbalance Sensitivity:** While the overall accuracy is acceptable, the Recall for the positive class (Plan Ultra) is only 22%. This means the model fails to identify nearly 80% of potential Ultra users. The high Precision (83%) for this class suggests that when the model predicts \"Ultra,\" it is usually correct, but it is being far too conservative.\n",
    "\n",
    "* **Path Forward:** The linear nature of this model limits its ability to capture complex patterns in user behavior. These results justify the transition to Ensemble Methods (Random Forest, Gradient Boosting), which are better suited to handle non-linear relationships and potentially improve the detection of the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2. 2nd Model: Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tuning & Evaluating: Random Forest ---\n",
      "Best Parameters: {'max_depth': 10, 'min_samples_leaf': 2, 'n_estimators': 200, 'random_state': 42}\n",
      "Training CV Accuracy: 0.8154\n",
      "Validation Accuracy:  0.7963\n",
      "\n",
      "Classification Report (Validation Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.91      0.86       446\n",
      "           1       0.73      0.53      0.62       197\n",
      "\n",
      "    accuracy                           0.80       643\n",
      "   macro avg       0.77      0.72      0.74       643\n",
      "weighted avg       0.79      0.80      0.79       643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter grid\n",
    "rf_params = {\n",
    "    'n_estimators': [200, 500], \n",
    "    'max_depth': [None, 10, 20], \n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'random_state': [42] # Ensuring reproducibility\n",
    "}\n",
    "\n",
    "# Running the master function\n",
    "best_rf = tune_and_evaluate(\n",
    "    RandomForestClassifier(),\n",
    "    rf_params,\n",
    "    X_train, y_train,\n",
    "    X_valid, y_valid,\n",
    "    model_name=\"Random Forest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interim Conclusions: Random Forest Classifier**\n",
    "The transition to an ensemble-based approach yielded a significant performance boost. Key observations from the Random Forest model include:\n",
    "\n",
    "* **Significant Predictive Gain:** The model reached a Validation Accuracy of 79.63%, a substantial improvement over the Logistic Regression baseline. This confirms that user behavior (calls, messages, and data usage) follows non-linear patterns that decision trees capture more effectively.\n",
    "\n",
    "* **Drastic Improvement in Recall:** The Recall for the 'Ultra' plan (Class 1) jumped from 22% to 53%. While there is still room for improvement, the model is now identifying more than half of the target users, making it much more viable for a targeted marketing campaign.\n",
    "\n",
    "* **Balanced Precision-Recall:** With a Precision of 73% for the positive class, the model maintains a high degree of confidence when it flags a user for a plan upgrade, minimizing \"false alarms\" that could annoy customers.\n",
    "\n",
    "* **Hyperparameter Insights:** The optimal `max_depth: 10` and `min_samples_leaf: 2` indicate that a moderately constrained tree structure is ideal for this dataset, providing enough complexity to learn but enough restriction to avoid overfitting, as evidenced by the narrow gap between Training (81.54%) and Validation (79.63%) scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modelo 3: Árbol de Decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Ajuste de Hiperparámetros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tuning & Evaluating: Decision Tree ---\n",
      "Best Parameters: {'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 10, 'random_state': 42}\n",
      "Training CV Accuracy: 0.7899\n",
      "Validation Accuracy:  0.7932\n",
      "\n",
      "Classification Report (Validation Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.94      0.86       446\n",
      "           1       0.78      0.45      0.57       197\n",
      "\n",
      "    accuracy                           0.79       643\n",
      "   macro avg       0.79      0.70      0.72       643\n",
      "weighted avg       0.79      0.79      0.77       643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Application: Decision Tree Classifier ---\n",
    "\n",
    "# Define the hyperparameter grid for Decision Tree\n",
    "dt_params = {\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'random_state': [42] # Consistency with previous experiments\n",
    "}\n",
    "\n",
    "# Running the master function\n",
    "best_dt = tune_and_evaluate(\n",
    "    DecisionTreeClassifier(),\n",
    "    dt_params,\n",
    "    X_train, y_train,\n",
    "    X_valid, y_valid,\n",
    "    model_name=\"Decision Tree\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conclusiones parciales - Árbol de Decisión:**\n",
    "\n",
    "* El árbol de decisión alcanza un accuracy ≈ 0.80 en test, con desempeño estable entre validación y test.\n",
    "\n",
    "* La profundidad limitada (max_depth = 5) y un min_samples_split = 10 ayudan a controlar el overfitting.\n",
    "\n",
    "* El modelo presenta alta precisión y recall para la clase 0, pero dificultades para capturar la clase 1 (recall = 0.43).\n",
    "\n",
    "* Aunque mejora a la regresión logística, queda por debajo del Random Forest en desempeño global.\n",
    "\n",
    "* Funciona como un modelo interpretable, pero no como la mejor opción final en términos predictivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Modelo 4: Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Ajuste de Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 300}\n",
      "Accuracy CV: 0.8003176098512886\n"
     ]
    }
   ],
   "source": [
    "# Modelo base\n",
    "gb = GradientBoostingClassifier()\n",
    "gb_params = {\n",
    "    'n_estimators':[300], \n",
    "    'learning_rate':[0.05,0.1], \n",
    "    'max_depth':[3,5]\n",
    "}\n",
    "\n",
    "# Grilla de hiperparámetros\n",
    "gb_grid = GridSearchCV(\n",
    "    gb, \n",
    "    gb_params, \n",
    "    cv=5, \n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# GridSearch\n",
    "gb_grid.fit(X_train, y_train)\n",
    "\n",
    "# Entrenar\n",
    "best_gb = gb_grid.best_estimator_\n",
    "\n",
    "# Resultados CV\n",
    "print('Mejores hiperparámetros:', gb_grid.best_params_)\n",
    "print('Accuracy CV:', gb_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en validación: 0.792\n"
     ]
    }
   ],
   "source": [
    "# Accuracy en VALIDACIÓN\n",
    "val_acc_gb = accuracy_score(y_valid, best_gb.predict(X_valid))\n",
    "print(f'Accuracy en validación: {val_acc_gb:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en test: 0.798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.92      0.86       446\n",
      "           1       0.75      0.51      0.61       197\n",
      "\n",
      "    accuracy                           0.80       643\n",
      "   macro avg       0.78      0.72      0.74       643\n",
      "weighted avg       0.79      0.80      0.79       643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Accuracy + métricas en TEST\n",
    "test_acc_gb = accuracy_score(y_test, best_gb.predict(X_test))\n",
    "\n",
    "print(f'Accuracy en test: {test_acc_gb:.3f}')\n",
    "print(classification_report(y_test, best_gb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conclusiones parciales - Gradient Boosting:**\n",
    "\n",
    "* Gradient Boosting logra un accuracy ≈ 0.82 en test, con resultados consistentes entre CV, validación y test.\n",
    "\n",
    "* El uso de learning_rate bajo (0.05) y árboles poco profundos (max_depth = 3) favorece una buena generalización.\n",
    "\n",
    "* Presenta muy buen desempeño en la clase 0 y una mejora clara en recall de la clase 1 respecto a regresión logística y árbol simple.\n",
    "\n",
    "* Su performance es similar al Random Forest, aunque ligeramente inferior en recall de la clase minoritaria.\n",
    "\n",
    "* Se posiciona como un modelo competitivo y robusto, aunque no el mejor global del experimento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Comparación de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_acc_log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModelo\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogistic Regression\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDecission Tree\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGradient Boosting\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidación\u001b[39m\u001b[38;5;124m'\u001b[39m:[val_acc_log, val_acc_rf, val_acc_dt, val_acc_gb],\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m'\u001b[39m:[test_acc_log, test_acc_rf, test_acc_dt, test_acc_gb]\n\u001b[0;32m      5\u001b[0m })\n\u001b[0;32m      7\u001b[0m results\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_acc_log' is not defined"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Modelo':['Logistic Regression','Random Forest', 'Decission Tree', 'Gradient Boosting'],\n",
    "    'Validación':[val_acc_log, val_acc_rf, val_acc_dt, val_acc_gb],\n",
    "    'Test':[test_acc_log, test_acc_rf, test_acc_dt, test_acc_gb]\n",
    "})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conclusiones Parciales: Comparación de Modelos**\n",
    "\n",
    "* **Random Forest es el mejor modelo**: obtiene el mayor accuracy en validación y en test, mostrando buena generalización.\n",
    "\n",
    "* **Gradient Boosting** queda muy cerca, con resultados sólidos y estables, aunque levemente por debajo del Random Forest.\n",
    "\n",
    "* **Decision Tree mejora** claramente a la Regresión Logística, pero queda limitado frente a los modelos ensemble.\n",
    "\n",
    "* **Logistic Regression** funciona como buen baseline, pero su desempeño es el más bajo en ambos conjuntos.\n",
    "\n",
    "* En general, **los modelos ensemble (RF y GB) superan a los modelos más simples**, confirmando que capturan mejor relaciones no lineales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Prueba de Cordura del Mejor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor modelo según test: Random Forest\n"
     ]
    }
   ],
   "source": [
    "# Seleccionamos el modelo con mejor accuracy en test\n",
    "best_row = results.loc[results['Test'].idxmax()]\n",
    "best_model_name = best_row['Modelo']\n",
    "\n",
    "# Diccionario modelo → objeto entrenado\n",
    "best_models_dict = {\n",
    "    'Logistic Regression': best_log,\n",
    "    'Random Forest': best_rf,\n",
    "    'Decission Tree': best_dt,\n",
    "    'Gradient Boosting': best_gb\n",
    "}\n",
    "\n",
    "# Recuperamos el mejor modelo\n",
    "best_model = best_models_dict[best_model_name]\n",
    "\n",
    "print(f'Mejor modelo según test: {best_model_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor modelo: Random Forest | Accuracy permutado: 0.593\n"
     ]
    }
   ],
   "source": [
    "# Test de Cordura\n",
    "# generador aleatorio reproducible\n",
    "rng = default_rng(42)\n",
    "\n",
    "# permutar índices del set de test\n",
    "perm = rng.permutation(len(X_test))\n",
    "\n",
    "# predicciones con datos desordenados\n",
    "predictions_permuted = best_model.predict(X_test[perm])\n",
    "\n",
    "# accuracy con etiquetas originales\n",
    "sanity_acc = accuracy_score(y_test, predictions_permuted)\n",
    "\n",
    "# resultado del sanity check\n",
    "print(\n",
    "    f'Mejor modelo: {best_model_name} | '\n",
    "    f'Accuracy permutado: {sanity_acc:.3f}'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conclusiones parciales: Prueba de Cordura**\n",
    "\n",
    "* Al permutar el conjunto de test, el accuracy cae de ~0.82 a ~0.59, muy cerca del azar.\n",
    "\n",
    "* Esto indica que el modelo depende de la relación real entre features y etiquetas, y no de patrones espurios.\n",
    "\n",
    "* El Random Forest no memoriza el orden de los datos, sino que aprende estructura.\n",
    "\n",
    "* La prueba de cordura valida que el buen desempeño previo es genuino.\n",
    "\n",
    "* El modelo supera claramente al azar solo cuando los datos están correctamente alineados, como se espera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Conclusión Final\n",
    "\n",
    "Se evaluaron **cuatro modelos de clasificación** (Logistic Regression, Decision Tree, Random Forest y Gradient Boosting) para predecir la variable objetivo, utilizando una metodología robusta con validación cruzada, set de validación y test independiente.\n",
    "\n",
    "* El **Random Forest** fue el modelo con mejor desempeño general, alcanzando el mayor accuracy en test (~0.82) y un balance más sólido entre precisión y recall en ambas clases.\n",
    "\n",
    "* Los modelos basados en **árboles** superaron claramente a la **regresión logística**, lo que sugiere la presencia de **relaciones no lineales** en los datos.\n",
    "\n",
    "* La **prueba de cordura** confirmó que el desempeño del mejor modelo **no es producto del azar**, validando la **capacidad predictiva real** del enfoque utilizado.\n",
    "\n",
    "* En función de los resultados, el **Random Forest** se selecciona como **modelo final recomendado**, combinando **buen rendimiento, estabilidad y generalización.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The Random Forest model outperformed other architectures with an accuracy of 82%, effectively identifying potential 'Ultra' plan adopters. This model could be integrated into the CRM to automate personalized plan recommendations, potentially increasing the conversion rate by identifying high-usage users more accurately than manual segmentation.\""
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 796,
    "start_time": "2026-01-10T19:39:46.832Z"
   },
   {
    "duration": 226,
    "start_time": "2026-01-10T19:41:13.725Z"
   },
   {
    "duration": 739,
    "start_time": "2026-01-10T19:41:20.371Z"
   },
   {
    "duration": 10,
    "start_time": "2026-01-10T19:41:21.112Z"
   },
   {
    "duration": 10,
    "start_time": "2026-01-10T19:41:21.123Z"
   },
   {
    "duration": 21,
    "start_time": "2026-01-10T19:41:21.135Z"
   },
   {
    "duration": 10,
    "start_time": "2026-01-10T19:41:21.158Z"
   },
   {
    "duration": 69,
    "start_time": "2026-01-10T19:41:21.190Z"
   },
   {
    "duration": 7,
    "start_time": "2026-01-10T19:41:21.261Z"
   },
   {
    "duration": 49514,
    "start_time": "2026-01-10T19:41:21.270Z"
   },
   {
    "duration": 128,
    "start_time": "2026-01-10T19:42:10.786Z"
   },
   {
    "duration": 616,
    "start_time": "2026-01-10T19:42:10.916Z"
   },
   {
    "duration": 4,
    "start_time": "2026-01-10T19:42:11.533Z"
   },
   {
    "duration": 8,
    "start_time": "2026-01-10T19:42:11.539Z"
   },
   {
    "duration": 763,
    "start_time": "2026-01-10T19:42:35.041Z"
   },
   {
    "duration": 8,
    "start_time": "2026-01-10T19:42:35.806Z"
   },
   {
    "duration": 10,
    "start_time": "2026-01-10T19:42:35.815Z"
   },
   {
    "duration": 21,
    "start_time": "2026-01-10T19:42:35.827Z"
   },
   {
    "duration": 10,
    "start_time": "2026-01-10T19:42:35.851Z"
   },
   {
    "duration": 94,
    "start_time": "2026-01-10T19:42:35.862Z"
   },
   {
    "duration": 3,
    "start_time": "2026-01-10T19:42:35.958Z"
   },
   {
    "duration": 6,
    "start_time": "2026-01-10T19:42:35.963Z"
   },
   {
    "duration": 49449,
    "start_time": "2026-01-10T19:42:35.971Z"
   },
   {
    "duration": 42,
    "start_time": "2026-01-10T19:43:25.422Z"
   },
   {
    "duration": 98,
    "start_time": "2026-01-10T19:43:25.465Z"
   },
   {
    "duration": 615,
    "start_time": "2026-01-10T19:43:25.564Z"
   },
   {
    "duration": 4,
    "start_time": "2026-01-10T19:43:26.180Z"
   },
   {
    "duration": 7,
    "start_time": "2026-01-10T19:43:26.191Z"
   },
   {
    "duration": 11,
    "start_time": "2026-01-10T19:46:44.296Z"
   },
   {
    "duration": 4,
    "start_time": "2026-01-10T19:47:33.914Z"
   },
   {
    "duration": 12915,
    "start_time": "2026-01-10T19:52:02.813Z"
   },
   {
    "duration": 7,
    "start_time": "2026-01-10T19:56:03.745Z"
   },
   {
    "duration": 13,
    "start_time": "2026-01-10T19:56:21.151Z"
   },
   {
    "duration": 7,
    "start_time": "2026-01-10T19:58:12.872Z"
   },
   {
    "duration": 226,
    "start_time": "2026-01-10T20:02:52.831Z"
   },
   {
    "duration": 5,
    "start_time": "2026-01-10T20:06:04.089Z"
   },
   {
    "duration": 14,
    "start_time": "2026-01-10T20:10:50.621Z"
   },
   {
    "duration": 831,
    "start_time": "2026-01-11T19:07:09.912Z"
   },
   {
    "duration": 10,
    "start_time": "2026-01-11T19:07:10.746Z"
   },
   {
    "duration": 10,
    "start_time": "2026-01-11T19:07:10.758Z"
   },
   {
    "duration": 35,
    "start_time": "2026-01-11T19:07:10.769Z"
   },
   {
    "duration": 10,
    "start_time": "2026-01-11T19:07:10.808Z"
   },
   {
    "duration": 85,
    "start_time": "2026-01-11T19:07:10.819Z"
   },
   {
    "duration": 4,
    "start_time": "2026-01-11T19:07:10.905Z"
   },
   {
    "duration": 9,
    "start_time": "2026-01-11T19:07:10.910Z"
   },
   {
    "duration": 49344,
    "start_time": "2026-01-11T19:07:10.921Z"
   },
   {
    "duration": 50,
    "start_time": "2026-01-11T19:08:00.267Z"
   },
   {
    "duration": 90,
    "start_time": "2026-01-11T19:08:00.319Z"
   },
   {
    "duration": 613,
    "start_time": "2026-01-11T19:08:00.410Z"
   },
   {
    "duration": 4,
    "start_time": "2026-01-11T19:08:01.025Z"
   },
   {
    "duration": 8,
    "start_time": "2026-01-11T19:08:01.031Z"
   },
   {
    "duration": 12645,
    "start_time": "2026-01-11T19:08:01.040Z"
   },
   {
    "duration": 16,
    "start_time": "2026-01-11T19:08:13.687Z"
   },
   {
    "duration": 13,
    "start_time": "2026-01-11T19:08:13.705Z"
   },
   {
    "duration": 8,
    "start_time": "2026-01-11T19:08:13.720Z"
   },
   {
    "duration": 4,
    "start_time": "2026-01-11T19:08:13.730Z"
   },
   {
    "duration": 67,
    "start_time": "2026-01-11T19:08:13.736Z"
   }
  ],
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
